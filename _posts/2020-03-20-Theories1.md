### GAN math

[Gan math1](https://github.com/mxahan/PDFS_notes/blob/master/Gan_math.pdf)

### Recurrent Neural Network

RNN learns while training and remember things learns from prior inputs while generating outputs.

<img src = "https://latex.codecogs.com/gif.latex?Output&space;=&space;f(input,&space;hiddenstate)">

RNN takes Series of input to produce a series of output vectors (No preset limilation on size).
![image RNN](https://ds055uzetaobb.cloudfront.net/brioche/uploads/Pxl5HYzpqr-rnn_small.png?width=2000)
Output
<img src = "https://latex.codecogs.com/gif.latex?o^t&space;=&space;f(h^t&space;;\theta)">
hiddenstate
<img src = "https://latex.codecogs.com/gif.latex?h^t&space;=&space;g(h^{t-1},x&space;;\theta)">

Unrolled Version,

![imge unrolled](https://ds055uzetaobb.cloudfront.net/brioche/uploads/fRVnZm2yoe-rnn_unfolded.png?width=2000)

Optimization algorithm: Back propagation through time (BPTT). Faces vanishing gradient problem.
**Parameter sharing**

**Deep RNNs**

**Bidirectional RNNs**

**Recursive Neural Network**

**Encoder Decoder Sequence to Sequence RNNs**

**LSTM**
Modified form of RNN to avoid vanishing gradient.
![image lstm](https://ds055uzetaobb.cloudfront.net/brioche/uploads/rZ6Bf7zO9Z-lstm.png?width=1200)
