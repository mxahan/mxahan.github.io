---
layout: post
title: "Theories1"
categories: Math
---

So far In this writing I have covered

- [GAN](#gan-math)
- [RNN](#recurrent-Neural-network)
- [Attention](#attention)
- [Normalizing Flows](#normalizing-flows)


# GAN math

I am covering some of the GAN maths from my old notes.

The informative PDF[Gan math1](https://github.com/mxahan/PDFS_notes/blob/master/Gan_math.pdf)

# Recurrent Neural Network

RNN learns while training and remember things learns from prior inputs while generating outputs.

<img src="https://latex.codecogs.com/gif.latex?Output&space;=&space;f(input,&space;hiddenstate)">

RNN takes Series of input to produce a series of output vectors (No preset limilation on size).

<img src="https://ds055uzetaobb.cloudfront.net/brioche/uploads/Pxl5HYzpqr-rnn_small.png?width=2000" width= "150">

Output
<img src="https://latex.codecogs.com/gif.latex?o^t&space;=&space;f(h^t&space;;\theta)">
hiddenstate

<img src="https://latex.codecogs.com/gif.latex?h^t&space;=&space;g(h^{t-1},x&space;;\theta)">

Unrolled Version,

<img src="https://ds055uzetaobb.cloudfront.net/brioche/uploads/fRVnZm2yoe-rnn_unfolded.png?width=2000" width="300">

Optimization algorithm: Back propagation through time (BPTT). Faces vanishing gradient problem.
**Parameter sharing**

**Deep RNNs**

**Bidirectional RNNs**

**Recursive Neural Network**

**Encoder Decoder Sequence to Sequence RNNs**

**LSTM**
Modified form of RNN to avoid vanishing gradient.

<img src="https://ds055uzetaobb.cloudfront.net/brioche/uploads/rZ6Bf7zO9Z-lstm.png?width=1200" width = "400">

An elaborate [tutorial with equations](https://www.cs.toronto.edu/~tingwuwang/rnn_tutorial.pdf).

A step by step [tutorial of LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

Understanding the [diagram](https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714)

# Attention

This writing consists of the personalized summary of Attention [blog](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) by Lilianweng. Great writer, a personal inspiration. Hope to put my understanding along with his contents.

The primary problems with the seq2seq models are the context vectors limitation.

<img src="https://lilianweng.github.io/lil-log/assets/images/encoder-decoder-attention.png" width= "500">

Figure: Source By Bahdanau et al., 2015

Three key points to remember
  - There is an context vector decides on hiddenstates of encoder
  - Hidden states weights decides context vector which is calculated by score function and softmax. This calculation leads to a family of context vectors.
  - Previous state, output and context vector determine next ouput.

Here are some keywords and terms we should distinguish with each other
 - Self attention: Correlation between current words and previous words
 - Soft vs Hard attention- Types of weights distribution over the patches
 -  Global vs local:

<img src = "https://lilianweng.github.io/lil-log/assets/images/luong2015-fig2-3.png" width = "500">

Figure: Global vs local attention source from *Luong, et al., 2015*

#### Neural Turing Machine
<img src ="https://lilianweng.github.io/lil-log/assets/images/NTM-flow-addressing.png" width = "500">
Figure: NTM addressing mechanism from Source

####  Transformer
 Seq2seq without recurrent unit. The model architectures are cleverly designed

- Key, value and query: Scaled dot product attention. The encoding of inputs are represented by Key Value pairs (V, K). In the decoder the previous output is compressed into a query (Q).

<img src = "https://latex.codecogs.com/gif.latex?Attention(Q,K,V)=softmax(QK^T/\sqrt{n})V">

- Multi-Head Self-attention

<img src="https://lilianweng.github.io/lil-log/assets/images/multi-head-attention.png" width= "300">

Figure: source *vaswani, et al, 2017*

Here the key point is the weighting of the V,K, Q to get the final attention.
-  Encoder Structure


<img src ="https://lilianweng.github.io/lil-log/assets/images/transformer-encoder.png" width= "300">

Figure: Source *Vaswani, et al., 2017*

- Decoder Structure

<img src="https://lilianweng.github.io/lil-log/assets/images/transformer-decoder.png" width = "400">

Figure: Source *Vaswani, et al. 2017*

- Final architecture

<img src="https://lilianweng.github.io/lil-log/assets/images/transformer.png" width = "500">

Figure: Source *Vaswani et al., 2017*

#### SNAIL

Solves the positioning problem in the transformer models.

<img src="https://lilianweng.github.io/lil-log/assets/images/snail.png" width="500">

Figure: Source *Mishra et al., 2017*

#### Self-Attention GAN

<img src="https://lilianweng.github.io/lil-log/assets/images/conv-vs-self-attention.png" width="500">

Figure: Convolution operation and self-attention Source *Zhang et al. 2018*

It has similar concept like key, value and query (f, g, h).

<img src="https://lilianweng.github.io/lil-log/assets/images/self-attention-gan-network.png" width="500">

Figure: Self-attention source *Zhang et al., 2018*


# Normalizing Flows

Motivational [blog1](https://blog.evjang.com/2018/01/nf1.html) and [blog2](https://blog.evjang.com/2018/01/nf2.html)

Normalizing flows are techniques to transform simple distribution to complex distribution. This ideas are entangled with the invertible transformation of densities. These transformation combinely forms Normalizing Flows

Estimating parametric distribution helps significantly in
- 1. Generate new data
- 2. Evaluate likelihood of models
- 3. Conditional distribution estimation
- 4. Score the algorithm be measuring entropy, MI, and moments of distribution.

There has been lack of attention in the (2., 3. and 4.) of the previous points.

The Question is to feasibility of finding distribution with following properties:
- Complex and rich enough to model multi-modal data analysis like images/value function
- Retain the easy comforts of Normal distribution!

The solution is:
- Mixture models with re-parameterization relaxation.
- Autoregressive factorizations of the policy/Value function
- In RL symmetry breaking value distribution via recurrent policies, noise or distributional RL
- Learning Energy-based models - Undirected graph with potential function.
- **Normalizing Flows** - learning invertible, volume tracking transformation for distributions to manipulate easily.

#### Change of variables, change of volume
Lets go over some points
- Linear transformation of points is a spacial case of y=f(x); x, y are high dimensionals vectors (We will talk about f(x)  in general as transformation)
- PDF and CDF
- Jacobian, Jacobian of inverse function and Determinant - Change in volume by transformation.
Consider x as Random variable and

<img src="https://latex.codecogs.com/gif.latex?x=f(y)">

Now PDF
<img src="https://latex.codecogs.com/gif.latex?p(y)=p(f^{-1}(y)).det(J(f^{-1}(y)))">
Taking log
<img src="https://latex.codecogs.com/gif.latex?\log{p(y)}=\log(p(f^{-1}(y))+\log{det(J(f^{-1}(y)))}">

#### Autoregressive Models are Normalizing Flows

A density estimation technique to represent complex joint densites as product of conditional distributions.

 <img src="https://latex.codecogs.com/gif.latex?p(x_{1:D})=\prod_ip(x_i|x_{i-1})">

For common choices, we have

<img src="https://latex.codecogs.com/gif.latex?p(x_i|x_{i-1})=\mathcal{N}(x_i|\mu_i,\exp(\alpha_i)^2)">

Where,

<img src="https://latex.codecogs.com/gif.latex?\mu_i=f_{\mu_i}(x_{1:i-1})">

and

<img src="https://latex.codecogs.com/gif.latex?\alpha_i=f_{\alpha_i}(x_{1:i-1})">

Here the inductive bias tells that earlier variables don't depend on later variables! To sample from the distribution to get data x from 1:D

<img src="https://latex.codecogs.com/gif.latex?x_i=u_i\exp\alpha_i+\mu_i">

where
<img src="https://latex.codecogs.com/gif.latex?u_ifrom\mathcal{N}(0,1)">

<img src="https://1.bp.blogspot.com/-sVVtT3hM65U/Wl-h3S4XqmI/AAAAAAAA
Hks/WKKYZvx4iw0TTwBLEcYdy3ceN7DKnzvnQCLcBGAs/s400/autoregressive.png">
Figure: Graphical View [source](https://blog.evjang.com/2018/01/nf2.html)

Here the learnable parameters are alpha's and mu's by training neural network with data. and the inverse

<img src="https://4.bp.blogspot.com/-Ev3O01VRJ_A/Wl-h-YXocxI/AAAAAAAAHk4/l5LKQwXvqKsNv4eR7AWX9Gt1TPeJrviQQCLcBGAs/s400/autoregressive_inv.png">\

Figure: Earlier [Source](https://blog.evjang.com/2018/01/nf2.html)


#### Inverse Autoregressive Flows(IAF)

Two major change in equations from the Masked AF earlier

<img src="https://latex.codecogs.com/gif.latex?p(x_i|x_{i-1})=\mathcal{N}(x_i|\mu_i,\exp(\alpha_i)^2)">

Where,

<img src="https://latex.codecogs.com/gif.latex?\mu_i=f_{\mu_i}(u_{1:i-1})">

and

<img src="https://latex.codecogs.com/gif.latex?\alpha_i=f_{\alpha_i}(u_{1:i-1})">

Mind the disinction between u and mu

<img src="https://3.bp.blogspot.com/-O5R7gBA4Qu8/Wl-iJMMpDvI/AAAAAAAAHk8/77uqTrQlug4XKkhuCXi-vDm5Beflin_vgCLcBGAs/s400/iaf.png" width="450">

Figure: [Source](https://blog.evjang.com/2018/01/nf2.html) Looks like inverse pass of MAF with changed labels. Extract u's from the x's using alpha's and mu's.

MAF (trains quickly but slow sample) and IAF (trains slow but fast samples) are trade of each other.

#### Parallel Wavenet

Combination of MAF and IAF is done in Parallel wavenet. It uses that idea that IAF can compute likelihood of own data cheaply but not for external data. MAF learns to generate likelihood of the external data.  

<img src="https://1.bp.blogspot.com/-EyDXE78u6ec/Wl_bTMXsl2I/AAAAAAAAHmA/J7CKHmubbwwwGD1KJNthdw2xnk0ZaLiWgCLcBGAs/s1600/pdd.png" width=500>

Figure: [Source](https://blog.evjang.com/2018/01/nf2.html) Here MAF (teacher) trains to generate samples and IAF (student) tries to generate samples with close match with teacher's likelihood of the IAF generated samples.

#### Real-NVP (Non-volume preserving)
Another recent network

<img src="https://3.bp.blogspot.com/-jnDFQyJAqoE/Wl-ivAo3IjI/AAAAAAAAHlI/37LM7EF1x1IlXLH6yH7jmkv1ODUSsaaGwCLcBGAs/s400/real_nvp.png" width="400">

Figure: Special case fo the IAF as 1:d in both x and u are equivalent. [Source](https://blog.evjang.com/2018/01/nf2.html)
