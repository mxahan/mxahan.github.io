---
layout: post
title: "Theories1"
categories: Math
---

# GAN math

[Gan math1](https://github.com/mxahan/PDFS_notes/blob/master/Gan_math.pdf)

# Recurrent Neural Network

RNN learns while training and remember things learns from prior inputs while generating outputs.

<img src = "https://latex.codecogs.com/gif.latex?Output&space;=&space;f(input,&space;hiddenstate)">

RNN takes Series of input to produce a series of output vectors (No preset limilation on size).

![image RNN](https://ds055uzetaobb.cloudfront.net/brioche/uploads/Pxl5HYzpqr-rnn_small.png?width=2000)
Output
<img src = "https://latex.codecogs.com/gif.latex?o^t&space;=&space;f(h^t&space;;\theta)">
hiddenstate
<img src = "https://latex.codecogs.com/gif.latex?h^t&space;=&space;g(h^{t-1},x&space;;\theta)">

Unrolled Version,

![imge unrolled](https://ds055uzetaobb.cloudfront.net/brioche/uploads/fRVnZm2yoe-rnn_unfolded.png?width=2000)

Optimization algorithm: Back propagation through time (BPTT). Faces vanishing gradient problem.
**Parameter sharing**

**Deep RNNs**

**Bidirectional RNNs**

**Recursive Neural Network**

**Encoder Decoder Sequence to Sequence RNNs**

**LSTM**
Modified form of RNN to avoid vanishing gradient.

![image lstm](https://ds055uzetaobb.cloudfront.net/brioche/uploads/rZ6Bf7zO9Z-lstm.png?width=1200)

An elaborate [tutorial with equations](https://www.cs.toronto.edu/~tingwuwang/rnn_tutorial.pdf).

A step by step [tutorial of LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

Understanding the [diagram](https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714)

# Attention

This writing consists of the personalized summary of Attention [blog](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) by Lilianweng. Great writer, a personal inspiration. Hope to put my understanding along with his contents.

The primary problems with the seq2seq models are the context vectors limitation.

![img](https://lilianweng.github.io/lil-log/assets/images/encoder-decoder-attention.png)
Figure: Source By Bahdanau et al., 2015

Three key points to remember
  - There is an context vector decides on hiddenstates of encoder
  - Hidden states weights decides context vector which is calculated by score function and softmax. This calculation leads to a family of context vectors.
  - Previous state, output and context vector determine next ouput.

Here are some keywords and terms we should distinguish with each other
 - Self attention: Correlation between current words and previous words
 - Soft vs Hard attention- Types of weights distribution over the patches
 -  Global vs local:

![img](https://lilianweng.github.io/lil-log/assets/images/luong2015-fig2-3.png){width: 50px;}
Figure: Global vs local attention source from *Luong, et al., 2015*

#### Neural Turing Machine
![img](https://lilianweng.github.io/lil-log/assets/images/NTM-flow-addressing.png)
Figure: NTM addressing mechanism from Source

####  Transformer
 Seq2seq without recurrent unit. The model architectures are cleverly designed

- Key, value and query: Scaled dot product attention. The encoding of inputs are represented by Key Value pairs (V, K). In the decoder the previous output is compressed into a query (Q).

<img src = "https://latex.codecogs.com/gif.latex?Attention(Q,K,V)=softmax(QK^T/\sqrt{n})V">

- Multi-Head Self-attention

![img](https://lilianweng.github.io/lil-log/assets/images/multi-head-attention.png)
Figure: source *vaswani, et al, 2017*

Here the key point is the weighting of the V,K, Q to get the final attention.
-  Encoder


![img](https://lilianweng.github.io/lil-log/assets/images/transformer-encoder.png)
Figure: Source *Vaswani, et al., 2017*

- Decoder

![img](https://lilianweng.github.io/lil-log/assets/images/transformer-decoder.png)
Figure: Source *Vaswani, et al. 2017*

- Final architecture

![img](https://lilianweng.github.io/lil-log/assets/images/transformer.png)
Figure: Source *Vaswani et al., 2017*

#### SNAIL

Solves the positioning problem in the transformer models.

![img](https://lilianweng.github.io/lil-log/assets/images/snail.png)
Figure: Source *Mishra et al., 2017*

#### Self-Attention GAN


![img](https://lilianweng.github.io/lil-log/assets/images/conv-vs-self-attention.png)
Figure: Convolution operation and self-attention Source *Zhang et al. 2018*

It has similar concept like key, value and query (f, g, h).

![img](https://lilianweng.github.io/lil-log/assets/images/self-attention-gan-network.png)
Figure: Self-attention source *Zhang et al., 2018*
