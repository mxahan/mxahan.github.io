# Introduction

This blog contains various random terms we encounter in our research activities. Some of the terms may need extensive elaboration to explain properly. I will may be refer them on my other section like trending topics or some terms.

##### Latency machine learning

[Nice Introduction](#https://iq.opengenus.org/latency-ml/#:~:text=Latency%20and%20throughput%20can%20be%20used%20interchangeably.&text=Latency%20is%20for%20batch%20size,and%20depends%20on%20the%20system) and the [original source](https://www.tiriasresearch.com/wp-content/uploads/2018/05/TIRIAS-Research-NVIDIA-PLASTER-Deep-Learning-Framework.pdf) and finally the impact of different [values](https://www.nngroup.com/articles/response-times-3-important-limits/)


In short, its the time required by the model to infer from one data sample. It can be think of as a response time for batch size of 1. Obviously the smaller the better means faster response. There is another terms related to the latency which is Throughput [look at the unit image per seconds for CV application].

Well it may seem that throughput is directly inverse to the latency, which is true up to inversely proportional. Just remember that sometimes operation can be shared and paralleled which will increase the throughput not the latency. We can measure the throughput by considering the memory and parallelism [more](https://towardsdatascience.com/the-correct-way-to-measure-inference-time-of-deep-neural-networks-304a54e5187f).


##### Stop Gradient
