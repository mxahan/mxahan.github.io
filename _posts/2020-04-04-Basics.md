# Probabilistic Terminology

####   Entropy:
<img src = "https://latex.codecogs.com/gif.latex?H(X)=-\sum_iP_X(x_i)\log_2P_X(x_i)=E[-\log_2P_X(x_i)]">

Well look at that Beautiful equation till it satisfies the eye and mind. Two things to remember when talking about an Entropy. 1. Probability distribution (Think of PDF; sum constraint to 1) and 2. Expectation of Log of the distribution over the allowed range. BooM. We get entropy!. Of course negate it. As log of something less than one is negative.

Special property: Given a PDF, Expectation log of that PDF is minimum if we take expectation w.r.t. the PDF itself. <img src = "https://latex.codecogs.com/gif.latex?E_{x\sim p(x)}[\log p(x)]\leq E_{x\sim q(x)}[\log p(x)]"> A.k.a cross entropy of p(x) w.r.t. q(x) distribution. This doesn't provide anything on the limit of the entropy of q(x).


Some important things to remember

- *H(f(X)) <= H(X)*; Known function doesn't reduce entropy.
- *H(X,Y) <= H(X) + H(Y)*; Worst case the maximum entropy of x and y can be summed.



**Conditional Entropy**
<img src = "https://latex.codecogs.com/gif.latex?H(Y|X)=-\sum_{i,j}p(x_i,y_j)\log\frac{p(x_i,y_j)}{p(x_i)}">
The above may give a vibe about negative of KL divergence but it's 100% not KL divergence. As the p(x) is not a probability distribution over x,y [doing the summation math over x-y space p(x) may be well over 1; crap! This breaks the PDF constraint!] space but only over x. This changes everything as the ratio p(x,y)/p(x) is always less or equal to 1 [p(y|x)]. Please refer to mutual entropy for more clearance. p(x)p(y) is a distribution over x-y space [doing the math of sum over x,y; combinatorial multiplication of p(x)p(y) over the x,y space], so does p(x,y).


Some useful points (think of Venn diagram)
1. H(Y|X) = H(X,Y)-H(X) = H(X|Y) + H(Y)-H(X)
- *H(Y|X) <= H(Y)*
- *H(Y,X) = H(X) + H(Y) - I(X;Y)*
- *H(Y|X) = H(Y)* in case of independence



**Different properties**

**Cross Entropy**
<img src = "https://latex.codecogs.com/gif.latex?H(p,q)=-E_p[\log q]=H(p)+D_{KL}(p||q)">


#### Renyi Entropy

**properties**

#### Mutual Information
From Wikipedia:
<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4b37af4015d70867b9c50b9cf82e0fc3598b070c">

<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7f3385f4d779f062696c134223b5683e754a6f1c">

<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3ec77b23435ade3cf8b29af42f9a2aae83c9fc50">


Some Useful points
- *I(X;Y) = H(Y) - H(Y|X)*

#### Kullback-Leibler Divergence
<img src = "https://latex.codecogs.com/gif.latex?D_{KL}(P||Q)=\sum_{x\epsilon X}P(x)\log(\frac{P(x)}{Q(x)})>=0">

Another cool format
<img src = "https://latex.codecogs.com/gif.latex?D_{KL}(P||Q)=H(P,Q)- H(P)">

Don't confuse it with the conditional formula, this guy is joint. The uncertainty (Loosely entropy) will increase or at least stay same, meanwhile for conditional the uncertainty will decrease or at maximum stay same.  

Very much related to fisher information metrics. In another day [math](https://en.wikipedia.org/wiki/Fisher_information_metric)

Generalized formula:

<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9ddb77c51a070f34b367212f73101c6c048e6579">

**properties**



**f-divergence family**

From Wikipedia

<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ba011de2a4fae8e3a06a89f2a198819a353545c0">

f-divergence to other Divergence

| Divergence | corresponding f(t) |
|------------|--------------------|
|KL-divergence | t*logt |
| Reverse KL | -logt |
| Total variation distance| .5*abs(t-1) |

**Bregman divergence**

#### Maximum Likelihood

#### MAP

#### Expectation Maximization

nice [post](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa13/slides/Likelihood_EM_HMM_Kalman.pdf) another [one](https://medium.com/@jonathan_hui/machine-learning-expectation-maximization-algorithm-em-2e954cb76959)


![img](https://miro.medium.com/max/1400/1*ljod05NDHzGGkoAcL0ZX5Q.jpeg)
Figure: From source very intuitive

#### Evidence Lower Bound

<img src = "https://latex.codecogs.com/gif.latex?L(X)=H(Q)-H(Q;P(X,Z))=\sum_zQ(Z)\log(P(Z,X)-\sum_zQ(Z)\log(Q(Z))">

Always negative.

<img src = "https://latex.codecogs.com/gif.latex?\log P(X)-D_{KL}(Q(Z)||P(Z|X))=L(X)">


### Banach Fixed point theorem

Well, lets start with Metric space; a ordered pair (A,d)
- A non empty set
- Real valued distance function d: AxA to R, satisfies metric space axioms. for x, y \epsilon A
  - d(x,x)>0
  - d(x,y)+d(y,z)>=d(x,z)
  - d(x,y)= d(y,x)
  - x=/y; d(x,y)>0

Cauchy sequence: for complete metric space
 - <img src = "https://latex.codecogs.com/gif.latex?\forall \epsilon \in R_{>0}: \exists N \in N : \forall m, n \in N: m, n \ge N: \map d(x_n, x_m) < \epsilon">

Contraction Mapping:
  - function f; from one metric space to another metric space: d2(f(x1),f(x2))<=d1(x1,x2) ; f lipschitz continous with lipschitz constant less than 1

Uff, finally, for complete metric (M,d) space, if f: M to M be a contraction

Then, 0<=q<1: d(f(x),f(y))<=qd(x,y)

Then there are unique fixed point of f : f(x)=x

### Eigen Value

### Matrix Multiplication

### Markov Random Field

### Graph Definition
