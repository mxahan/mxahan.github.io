# Probabilistic Terminology

####   Entropy:
Distribution Alert!!!

<img src = "https://latex.codecogs.com/gif.latex?H(X)=-\sum_iP_X(x_i)\log_2P_X(x_i)=E[-\log_2P_X(x_i)]">

Well look at that Beautiful equation till it satisfies the eye and mind.It gives a value [point estimation]. Two things to remember when talking about an Entropy. 1. Probability distribution (Think of PDF; sum constraint to 1) and 2. Expectation of Log of the distribution over the allowed range. BooM. We get entropy!. Of course negate it. As log of something less than one is negative.

Special property: Given a PDF, Expectation log of that PDF is minimum if we take expectation w.r.t. the PDF itself. <img src = "https://latex.codecogs.com/gif.latex?E_{x\sim p(x)}[-\log p(x)]\leq E_{x\sim p(x)}[-\log q(x)]"> A.k.a cross entropy of q(x) w.r.t. p(x) distribution [notion of cross entropy]. This doesn't provide anything on the limit of the entropy of q(x).


Some important things to remember

- *H(f(X)) <= H(X)*; Known function doesn't reduce entropy.
- *H(X,Y) <= H(X) + H(Y)*; Worst case the maximum entropy of x and y can be summed.



**Conditional Entropy**
<img src = "https://latex.codecogs.com/gif.latex?H(Y|X)=-\sum_{i,j}p(x_i,y_j)\log\frac{p(x_i,y_j)}{p(x_i)}">
The above may give a vibe about negative of KL divergence but it's 100% not KL divergence. As the p(x) is not a probability distribution over x,y [doing the summation math over x-y space p(x) may be well over 1; crap! This breaks the PDF constraint!] space but only over x. This changes everything as the ratio p(x,y)/p(x) is always less or equal to 1 [p(y|x)]. Please refer to mutual entropy for more clearance. p(x)p(y) is a distribution over x-y space [doing the math of sum over x,y; combinatorial multiplication of p(x)p(y) over the x,y space], so does p(x,y).

Conditional entropy provides the entropy that knowing the conditioned variable won't reduce. H(X|Y) means how much entropy still remains for X given we know Y. It can be maximum of H(X) (independence) and minimum to 0 (certainty). H(X) contains sum of two distinct parts 1. MI and 2. Conditional entropy. This is expectation of logP(x|y). The conditional probability p(x|y) may increase or decrease from p(x) but the H(X|Y) will decrease or at max stay the same as H(X).

Some useful points (think of Venn diagram)
1. H(Y|X) = H(X,Y)-H(X) = H(X|Y) + H(Y)-H(X); Subtract the known information from the Total unknown
1. *H(Y|X) <= H(Y)* ; Knowing something can't increase the entropy.
1. *H(Y,X) = H(X) + H(Y) - I(X;Y)* ; Refer to MI section
1. *H(Y|X) = H(Y)* in case of independence



**Cross Entropy**
<img src = "https://latex.codecogs.com/gif.latex?H(p,q)=-E_p[\log q]=H(p)+D_{KL}(p||q)">
Cross entropy of **distribution** q(x) w.r.t **distribution** p(x) is always greater than entropy of p(x) [discussed earlier]. Whatever distribution is inside log() that's entropy we are calculating. We are just introducing more uncertainty! Cross entropy is minimum when p(x) and q(x) are same for all x, which is equal to entropy of q(x) or p(x) given, setting the kl terms to 0 by equating p and q distributions. Cross entropy provides a higher bound on entropy over p(x), nothing to do with entropy of q(x)[-log q(x) expected w.r.t q(x)], it can be 0 or very high though.

The **key focus upon the p(x) [which distribution is using for entropy calculation]**, as cross entropy w.r.t. p(x) would allow to know the higher bound of entropy of p(x). Knowing H(p,q) we know the upper bound for the H(p) and vice versa.  {discussed above}


#### Renyi Entropy

**properties**

#### Mutual Information
From Wikipedia:
<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4b37af4015d70867b9c50b9cf82e0fc3598b070c">

<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7f3385f4d779f062696c134223b5683e754a6f1c"> or

<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3ec77b23435ade3cf8b29af42f9a2aae83c9fc50">

This notation of joint entropy [H(X,Y) = E[log p(x,y)] w.r.t. distribution of p(x,y)] may be confused with the cross entropy H(p,q). Joint entropy takes two RV where as the cross entropy takes two distribution over the same RV.

This is another important and insightful equation with insight. The MI tells us mutual unknown between x and y together. Think of Venn diagram. MI indicates the region where both x,y intersects in terms of unknown. Alternatively, if we know x how much it helps to understand y, or vice versa. Since *if we knew* is there, this is still uncertain.

Quick derivation: If p(x), p(y) are independent the then there is no mutual information or MI=0, [Basic p(x,y)=p(x)p(y), in case of independence]. MI upper bound is minimum(H(x),H(y)); one is described by the others totally.

Some Useful points
1. *I(X;Y) = H(Y) - H(Y|X)*; Subtract the conditional entropy (unknown) from the total entropy (unknown), leaving how much we know about them simultaneously.
1. I(X;Y) = H(X) -H(X|Y)
1. I(X;Y) = H(X,Y) - H(X|Y) - H(Y|X)

#### Kullback-Leibler Divergence
<img src = "https://latex.codecogs.com/gif.latex?D_{KL}(P||Q)=\sum_{x\epsilon X}p(x)\log(\frac{p(x)}{q(x)})>=0">

Another commonly encountered term. Can be expressed as cross entropy of q w.r.t p and entropy of p terms (subtraction.). This can be interpreted as entropy overestimation by selecting *log q* instead of *log p*. As discussed above knowing E[logq] w.r.t p we already know the upper bound for E[logp]. Clearly minimum to 0 in case of p and q are same for all x.

Quick points: This is not a metric as it's not symmetric. This is in coherence that E_p[logq] has nothing to do with E_q[logq] (lower bound for E_q[logp]).

Another cool format

<img src = "https://latex.codecogs.com/gif.latex?D_{KL}(P||Q)=H(P,Q)- H(P)">

Don't confuse it with the conditional formula, that one was joint distribution. The uncertainty (Loosely entropy) will increase or at least stay same, meanwhile for conditional the uncertainty will decrease or at maximum stay same. Moreover conditional entropy has two RV and here is only one but two distributions.

Very much related to fisher information metrics. In another day [math](https://en.wikipedia.org/wiki/Fisher_information_metric)

Generalized formula:

<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9ddb77c51a070f34b367212f73101c6c048e6579">

**properties**



**f-divergence family**

From Wikipedia

<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ba011de2a4fae8e3a06a89f2a198819a353545c0">

f-divergence to other Divergence

| Divergence | corresponding f(t) |
|------------|--------------------|
|KL-divergence | t*logt |
| Reverse KL | -logt |
| Total variation distance| .5*abs(t-1) |

**Bregman divergence**

#### Maximum Likelihood

#### MAP

#### Expectation Maximization

nice [post](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa13/slides/Likelihood_EM_HMM_Kalman.pdf) another [one](https://medium.com/@jonathan_hui/machine-learning-expectation-maximization-algorithm-em-2e954cb76959)


![img](https://miro.medium.com/max/1400/1*ljod05NDHzGGkoAcL0ZX5Q.jpeg)
Figure: From source very intuitive

#### Evidence Lower Bound

[Nice intuitive link](http://legacydirs.umiacs.umd.edu/~xyang35/files/understanding-variational-lower.pdf)

This provides lower bound on logP(X) given the all data point observation X and an approximation of parametric distribution. It's always negative or at max 0. This bounds the log likelihood of data observation.

Two alternative formulation. One gives the actual bound and other gives the difference of log likelihood and the bound itself, kl of estimation q(z) and true posterior p(z|x). Here we assume the q(z) and the true posterior is unknown or we try to reach.

<img src = "https://latex.codecogs.com/gif.latex?L(X)=H(Q)-H(Q;P(X,Z))=\sum_zQ(Z)\log(P(Z,X))-\sum_zQ(Z)\log(Q(Z)">

Always negative.

<img src = "https://latex.codecogs.com/gif.latex?\log P(X)-D_{KL}(Q(Z)||P(Z|X))=L(X)">


### Banach Fixed point theorem

Well, lets start with Metric space; a ordered pair (A,d)
- A non empty set
- Real valued distance function d: AxA to R, satisfies metric space axioms. for x, y \epsilon A
  - d(x,x)>0
  - d(x,y)+d(y,z)>=d(x,z)
  - d(x,y)= d(y,x)
  - x=/y; d(x,y)>0

Cauchy sequence: for complete metric space
 - <img src = "https://latex.codecogs.com/gif.latex?\forall \epsilon \in R_{>0}: \exists N \in N : \forall m, n \in N: m, n \ge N:  d(x_n, x_m)<\epsilon">

Contraction Mapping:
  - function f; from one metric space to another metric space: d2(f(x1),f(x2))<=d1(x1,x2) ; f lipschitz continous with lipschitz constant less than 1

Uff, finally, for complete metric (M,d) space, if f: M to M be a contraction

Then, 0<=q<1: d(f(x),f(y))<=qd(x,y)

Then there are unique fixed point of f : f(x)=x

### Eigen Value

### Matrix Multiplication

### Markov Random Field

### Graph Definition
