# Probabilistic Terminology

####   Entropy:
Distribution Alert!!!

<img src = "https://latex.codecogs.com/gif.latex?H(X)=-\sum_iP_X(x_i)\log_2P_X(x_i)=E[-\log_2P_X(x_i)]">

Well look at that Beautiful equation till it satisfies the eye and mind.It gives a value [point estimation]. Two things to remember when talking about an Entropy. 1. Probability distribution (Think of PDF; sum constraint to 1) and 2. Expectation of Log of the distribution over the allowed range. BooM. We get entropy!. Of course negate it. As log of something less than one is negative.

Special property: Given a PDF, Expectation log of that PDF is minimum if we take expectation w.r.t. the PDF itself. <img src = "https://latex.codecogs.com/gif.latex?E_{x\sim p(x)}[-\log p(x)]\leq E_{x\sim p(x)}[-\log q(x)]"> A.k.a cross entropy of q(x) w.r.t. p(x) distribution [notion of cross entropy]. This doesn't provide anything on the limit of the entropy of q(x).


Some important things to remember

- *H(f(X)) <= H(X)*; Known function doesn't reduce entropy.
- *H(X,Y) <= H(X) + H(Y)*; Worst case the maximum entropy of x and y can be summed.



**Conditional Entropy**
<img src = "https://latex.codecogs.com/gif.latex?H(Y|X)=-\sum_{i,j}p(x_i,y_j)\log\frac{p(x_i,y_j)}{p(x_i)}">
The above may give a vibe about negative of KL divergence but it's 100% not KL divergence. As the p(x) is not a probability distribution over x,y [doing the summation math over x-y space p(x) may be well over 1; crap! This breaks the PDF constraint!] space but only over x. This changes everything as the ratio p(x,y)/p(x) is always less or equal to 1 [p(y:x)]. Please refer to mutual entropy for more clearance. p(x)p(y) is a distribution over x-y space [doing the math of sum over x,y; combinatorial multiplication of p(x)p(y) over the x,y space], so does p(x,y).

Conditional entropy provides the entropy that knowing the conditioned variable won't reduce. H(X:Y) [: sign to mean condition in this text] means how much entropy still remains for X given we know Y. It can be maximum of H(X) (independence) and minimum to 0 (certainty). H(X) contains sum of two distinct parts 1. MI and 2. Conditional entropy. This is expectation of logP(x:y). The conditional probability p(x:y) may increase or decrease from p(x) but the H(X:Y) will decrease or at max stay the same as H(X).

Some useful points (think of Venn diagram)
1. H(Y:X) = H(X,Y)-H(X) = H(X:Y) + H(Y)-H(X); Subtract the known information from the Total unknown
1. *H(Y:X) <= H(Y)* ; Knowing something can't increase the entropy.
1. *H(Y,X) = H(X) + H(Y) - I(X;Y)* ; Refer to MI section
1. *H(Y:X) = H(Y)* in case of independence



**Cross Entropy**
<img src = "https://latex.codecogs.com/gif.latex?H(p,q)=-E_p[\log q]=H(p)+D_{KL}(p||q)">
Cross entropy of **distribution** q(x) w.r.t **distribution** p(x) is always greater than entropy of p(x) [discussed earlier]. Whatever distribution is inside log() that's entropy we are calculating. We are just introducing more uncertainty! Cross entropy is minimum when p(x) and q(x) are same for all x, which is equal to entropy of q(x) or p(x) given, setting the kl terms to 0 by equating p and q distributions. Cross entropy provides a higher bound on entropy over p(x), nothing to do with entropy of q(x)[-log q(x) expected w.r.t q(x)], it can be 0 or very high though.

The **key focus upon the p(x) [which distribution is using for entropy calculation]**, as cross entropy w.r.t. p(x) would allow to know the higher bound of entropy of p(x). Knowing H(p,q) we know the upper bound for the H(p) and vice versa.  {discussed above}

[entropy comparison: who is what](http://colah.github.io/posts/2015-09-Visual-Information/)

#### Renyi Entropy

**properties**

#### Mutual Information
From Wikipedia:
<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4b37af4015d70867b9c50b9cf82e0fc3598b070c">

<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7f3385f4d779f062696c134223b5683e754a6f1c"> or

<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3ec77b23435ade3cf8b29af42f9a2aae83c9fc50">

This notation of joint entropy [H(X,Y) = E[log p(x,y)] w.r.t. distribution of p(x,y)] may be confused with the cross entropy H(p,q). Joint entropy takes two RV where as the cross entropy takes two distribution over the same RV.

This is another important and insightful equation with insight. The MI tells us mutual unknown between x and y together. Think of Venn diagram. MI indicates the region where both x,y intersects in terms of unknown. Alternatively, if we know x how much it helps to understand y, or vice versa. Since *if we knew* is there, this is still uncertain.

Quick derivation: If p(x), p(y) are independent the then there is no mutual information or MI=0, [Basic p(x,y)=p(x)p(y), in case of independence]. MI upper bound is minimum(H(x),H(y)); one is described by the others totally.

Some Useful points
1. *I(X;Y) = H(Y) - H(Y:X)*; Subtract the conditional entropy (unknown) from the total entropy (unknown), leaving how much we know about them simultaneously.
1. I(X;Y) = H(X) -H(X:Y)
1. I(X;Y) = H(X,Y) - H(X:Y) - H(Y:X)

#### Kullback-Leibler (KL) Divergence
<img src = "https://latex.codecogs.com/gif.latex?D_{KL}(P||Q)=\sum_{x\epsilon X}p(x)\log(\frac{p(x)}{q(x)})>=0">

Another commonly encountered term. Can be expressed as cross entropy of q w.r.t p and entropy of p terms (subtraction.). This can be interpreted as entropy overestimation by selecting *log q* instead of *log p*. As discussed above knowing E[-logq] w.r.t p we already know the upper bound for E[-logp]. Clearly minimum to 0 in case of p and q are same for all x.

Quick points: This is not a metric as it's not symmetric. This is in coherence that E_p[logq] has nothing to do with E_q[logq] (Upper bound for E_q[logp]). [QuickNote: Look and understand the sign. E_p[logq] is lower bound for E_p[logp]; But E_p[-logq] is the upper bound for E_p[logp], so does the other combinaortial options.]

Two interesting view of the KL divergence [link](https://dibyaghosh.com/blog/probability/kldivergence.html)
  - Supervised Learning (forward KL) -  arg min D<sub>KL</sub>(P||Q<sub>theta</sub>)
  Mean seeking behavior; where p is high, Q must be high. [less penalty for low point missing of P by Q]
  Q should get P by mean [get all the high point for the P modes in expectation ]
  - Reinforcement learning (Reverse KL) - arg min D<sub>KL</sub>(Q<sub>theta</sub>||P)
  Mode Seeking behavior; Where Q is high P must have high too [less care when Q is low] - get the maximum mode for the P by Q. (as the equation log(value) multiplied by Q), So Q will place itself to get the maximum mode of P.

Another cool format

<img src = "https://latex.codecogs.com/gif.latex?D_{KL}(P||Q)=H(P,Q)- H(P)">

Don't confuse it with the conditional formula, that one was joint distribution. The uncertainty (Loosely entropy) will increase or at least stay same, meanwhile for conditional the uncertainty will decrease or at maximum stay same. Moreover conditional entropy has two RV and here is only one but two distributions.

Very much related to fisher information metrics. In another day [math](https://en.wikipedia.org/wiki/Fisher_information_metric)

Generalized formula:

<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9ddb77c51a070f34b367212f73101c6c048e6579">

**properties**



**f-divergence family**

From Wikipedia

<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ba011de2a4fae8e3a06a89f2a198819a353545c0">

f-divergence to other Divergence

| Divergence | corresponding f(t) |
|------------|--------------------|
|KL-divergence | t*logt |
| Reverse KL | -logt |
| Total variation distance| .5*abs(t-1) |

**Bregman divergence**

#### Maximum Likelihood

#### MAP

#### Expectation Maximization

nice [post](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa13/slides/Likelihood_EM_HMM_Kalman.pdf) another [one](https://medium.com/@jonathan_hui/machine-learning-expectation-maximization-algorithm-em-2e954cb76959)


![img](https://miro.medium.com/max/1400/1*ljod05NDHzGGkoAcL0ZX5Q.jpeg)
Figure: From source very intuitive

#### Evidence Lower Bound

[Nice intuitive link](http://legacydirs.umiacs.umd.edu/~xyang35/files/understanding-variational-lower.pdf)

This provides lower bound on logP(X) given the all data point observation X and an approximation of parametric distribution. It's always negative or at max 0. This bounds the log likelihood of data observation.

Two alternative formulation. One gives the actual bound and other gives the difference of log likelihood and the bound itself, kl of estimation q(z) and true posterior p(z|x). Here we assume the q(z) and the true posterior is unknown or we try to reach.

<img src = "https://latex.codecogs.com/gif.latex?L(X)=H(Q)-H(Q;P(X,Z))=\sum_zQ(Z)\log(P(Z,X))-\sum_zQ(Z)\log(Q(Z)">

Always negative.

<img src = "https://latex.codecogs.com/gif.latex?\log P(X)-D_{KL}(Q(Z)||P(Z|X))=L(X)">


### Banach Fixed point theorem

Well, lets start with Metric space; a ordered pair (A,d)
- A non empty set
- Real valued distance function d: AxA to R, satisfies metric space axioms. for x, y \epsilon A
  - d(x,x)>0
  - d(x,y)+d(y,z)>=d(x,z)
  - d(x,y)= d(y,x)
  - x=/y; d(x,y)>0

Cauchy sequence: for complete metric space
 - <img src = "https://latex.codecogs.com/gif.latex?\forall \epsilon \in R_{>0}: \exists N \in N : \forall m, n \in N: m, n \ge N:  d(x_n, x_m)<\epsilon">

Contraction Mapping:
  - function f; from one metric space to another metric space: d2(f(x1),f(x2))<=d1(x1,x2) ; f lipschitz continous with lipschitz constant less than 1

Uff, finally, for complete metric (M,d) space, if f: M to M be a contraction

Then, 0<=q<1: d(f(x),f(y))<=qd(x,y)

Then there are unique fixed point of f : f(x)=x

### Norms and Condition Number
[nice tutorial](http://www.cse.iitd.ernet.in/~dheerajb/CS210_lect07.pdf)
Vector norm: l2, l1 ,.. l-infinte norm. Definition.
<img src = "https://latex.codecogs.com/gif.latex?l_p=(\sum_{i=1}^nx_i^p)^{1/p}"> where x_i are the vector elements.

Think of a point in n dimentional space.

Matrix norm in little tricky
<img src = "https://latex.codecogs.com/gif.latex?||A||=max_{x\neq0}\frac{||Ax_v||}{x_v}"> where x_v symbolizes a vector.

Well this is a maximization. Of course l1 norm would be sum of maximum absolute column value [multiply one hot encoding vector and norm]. l_infine norm would be sum of rows [multiply by all one vector].

##### Condition Number
<img src = "https://latex.codecogs.com/gif.latex?cond(A)=||A||.||A^{-1}||"> Depends on which norm we are considering.

It also means the ratio of maximum and minimum stretching of some vectors [proof](https://blogs.mathworks.com/cleve/2017/07/17/what-is-the-condition-number-of-a-matrix/). Computation is tricky due to matrix inversion operation. For non singular l2 conditional number of a nonsingular matrix would be the ratio of maximum to minimum singular value of matrix. The more close to singular the higher condition number. [some extra](http://faculty.nps.edu/rgera/MA3042/2009/ch7.4.pdf)

### EigenValue

### Linear algebra Essentials
[blog post](https://medium.com/@jonathan_hui/machine-learning-linear-algebra-a5b1658f0151)

Vector scaler product is projection. Matrix multiplied with vector is the projection of vector in each of the row of the matrix. Well simple but very effective!! Just apply it next time we encounter some vector, vector or matrix, vector multiplication.

Combine multiplication with eigenvector value! or orthogonal or parallel.

### Markov Random Field

### Graph Definition

### Lipschitz


### LP duality

### PSD matrix

One thing that appears again and again. Let's dive into conceptual plane about what the Positive Semidefinite matrix. Well, simply, this is a property of matrix Norm with symmetry condition. Two things to come immediately with PSD concept
  - M is symmetric
  - v<sup>T</sup>Mv >= 0
It follows that when the 2nd condition is always strictly greater than 0 it is call positive definite matrix. and < 0, means negative definite matrix.  

As I promised to go beyond the definition, lets break the 1st condition. Hmm, Symmetric Matrix. Row, Columns are same. Lots of other things [follow](https://en.wikipedia.org/wiki/Symmetric_matrix) or [shorter](https://mathworld.wolfram.com/SymmetricMatrix.html). What hits me most is the property of eigenvalues for real symmetric matrix (spoiler alert: All eigenvalues are real) and diagonizable by the forming a matrix by taking the eigenvectors as rows [can be written  as weighted sum of vector multiplication (nxn)(nxn)(nxn) to nxn shape]. This can be extending to multiplication of two matrix (M=B<sup>T</sup>B) by reformulating the diagonizable equation [can be written  as weighted (all weight 1) sum of vector multiplication (nxn)(nxn) to nxn shape].

The second condition says, that **project (transform) any vector on the matrix rows and project it on the original vector would make it positive or atleast zero.** Means projection will not rotate the vector in the opposite direction. This automatically lead the property of the eigenvalues for PSD matrix (yes they will all be always positive), just shrink or expand it but not reverse the eigenvector (or any vector) by definition.  
[shorthand](https://www.cse.iitk.ac.in/users/rmittal/prev_course/s14/notes/lec11.pdf)

[more](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/positive-definite-matrices-and-applications/symmetric-matrices-and-positive-definiteness/MIT18_06SCF11_Ses3.1sum.pdf)


### FID

Use to evaluate Gan generator performance.

Mathematical [definition](https://nealjean.com/ml/frechet-inception-distance/)

First let's start with inception distance.

<img src = "https://latex.codecogs.com/gif.latex?IS=E_{x \sim P_g}D_{KL}(p(y|x)||p(y))">

As the equation states, it measures the distribution entropy of label (y) of the generator from the original. It just look at the label distribution! It should reduce the entropy of label given generated image [means we can understand the object from the generated image easily], label uncertainty reduce. In other words, E<sub>x~gen</sub>[-log p(y given x)] should be low. Secondly, the marginal distribution of y given x, w.r.t. x should match the real data label distribution.

The problem is that IS offers no statistical measurement. p(y given x) can be vague in the original image too! Then why we want to reduce it in the generated image. Moreover, p(y given x) can be misleading.

Here comes the FID, [lets assume the multivariate gaussian; yes we have to do it]. Now fit the multivariate gaussian to find the parameters for both the real and the generated image [of course after passing them for some further modification; inception V3 layer activation.]. <img src = "https://latex.codecogs.com/gif.latex?X_r\sim\mathcal{N}(\mu_r, \Sigma_r),X_g\sim\mathcal{N}(\mu_g, \Sigma_g)">.

Finally calculate; <img src = "https://latex.codecogs.com/gif.latex?FID=||\mu_r-\mu_g||^2+Tr(\Sigma_r+\Sigma_g- 2(\Sigma_r\Sigma_g)^{0.5})">

As expected the lower the better. They are measured in the inception layers activation map for the real and generated image.


### Loss functions

##### Cross entropy: <img src = "https://latex.codecogs.com/gif.latex?-\sum_{i=1}^cy_{true,i}\log y_{pred,i}">
Its derivative [formula](https://deepnotes.io/softmax-crossentropy). It's also related to [**focal loss**](https://medium.com/visionwizard/understanding-focal-loss-a-quick-read-b914422913e7).

##### Hinge Loss:  
