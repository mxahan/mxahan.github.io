# Probabilistic Terminology

####   Entropy:
<img src = "https://latex.codecogs.com/gif.latex?H(X)=-\sum_iP_X(x_i)\log_2P_X(x_i)=E[-\log_2P_X(x_i)]">

Some important things to remember

- *H(f(X)) <= H(X)*
- *H(X,Y) <= H(X) + H(Y)*



**Conditional Entropy**
<img src = "https://latex.codecogs.com/gif.latex?H(Y|X)=-\sum_{i,j}P_X(x_i,y_j)\log\frac{P_X(x_i,y_j)}{P(x_i)}">

Some useful points
- *H(Y|X) = H(X,Y)-H(X) = H(X|Y) + H(Y)-H(X)*
- *H(Y|X) <= H(Y)*
- *H(Y,X) = H(X) + H(Y) - I(X;Y)*
- *H(Y|X) = H(Y)* in case of independence



**Different properties**

**Cross Entropy**
<img src = "https://latex.codecogs.com/gif.latex?H(p,q)=-E_p[\log q]=H(p)+D_{KL}(p||q)">


#### Renyi Entropy

**properties**

#### Mutual Information
From Wikipedia:
<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4b37af4015d70867b9c50b9cf82e0fc3598b070c">

<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7f3385f4d779f062696c134223b5683e754a6f1c">

<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3ec77b23435ade3cf8b29af42f9a2aae83c9fc50">


Some Useful points
- *I(X;Y) = H(Y) - H(Y|X)*

#### Kullback-Leibler Divergence
<img src = "https://latex.codecogs.com/gif.latex?D_{KL}(P||Q)=\sum_{x\epsilon X}P(x)\log(\frac{P(x)}{Q(x)})>=0">

Another cool format
<img src = "https://latex.codecogs.com/gif.latex?D_{KL}(P||Q)=H(P,Q)- H(P)">

Don't confuse it with the conditional formula, this guy is joint. The uncertainty (Loosely entropy) will increase or at least stay same, meanwhile for conditional the uncertainty will decrease or at maximum stay same.  

Very much related to fisher information metrics. In another day [math](https://en.wikipedia.org/wiki/Fisher_information_metric)

Generalized formula:

<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9ddb77c51a070f34b367212f73101c6c048e6579">

**properties**



**f-divergence family**

From Wikipedia

<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ba011de2a4fae8e3a06a89f2a198819a353545c0">

f-divergence to other Divergence

| Divergence | corresponding f(t) |
|------------|--------------------|
|KL-divergence | t*logt |
| Reverse KL | -logt |
| Total variation distance| .5*abs(t-1) |

**Bregman divergence**

#### Maximum Likelihood

#### MAP

#### Expectation Maximization

nice [post](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa13/slides/Likelihood_EM_HMM_Kalman.pdf) another [one](https://medium.com/@jonathan_hui/machine-learning-expectation-maximization-algorithm-em-2e954cb76959)


![img](https://miro.medium.com/max/1400/1*ljod05NDHzGGkoAcL0ZX5Q.jpeg)
Figure: From source very intuitive

#### Evidence Lower Bound

<img src = "https://latex.codecogs.com/gif.latex?L(X)=H(Q)-H(Q;P(X,Z))=\sum_zQ(Z)\log(P(Z,X)-\sum_zQ(Z)\log(Q(Z))">

Always negative.

<img src = "https://latex.codecogs.com/gif.latex?\log P(X)-D_{KL}(Q(Z)||P(Z|X))=L(X)">


### Banach Fixed point theorem

Well, lets start with Metric space; a ordered pair (A,d)
- A non empty set
- Real valued distance function d: AxA to R, satisfies metric space axioms. for x, y \epsilon A
  - d(x,x)>0
  - d(x,y)+d(y,z)>=d(x,z)
  - d(x,y)= d(y,x)
  - x=/y; d(x,y)>0

Cauchy sequence: for complete metric space
 - <img src = "https://latex.codecogs.com/gif.latex?\forall \epsilon \in R_{>0}: \exists N \in N : \forall m, n \in N: m, n \ge N: \map d(x_n, x_m) < \epsilon">

Contraction Mapping:
  - function f; from one metric space to another metric space: d2(f(x1),f(x2))<=d1(x1,x2) ; f lipschitz continous with lipschitz constant less than 1

Uff, finally, for complete metric (M,d) space, if f: M to M be a contraction

Then, 0<=q<1: d(f(x),f(y))<=qd(x,y)

Then there are unique fixed point of f : f(x)=x

### Eigen Value

### Matrix Multiplication 

### Markov Random Field

### Graph Definition
